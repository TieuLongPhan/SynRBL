{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from SynRBL.rsmi_utils import load_database\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from SynAnalysis.analysis_utils import remove_atom_mapping_from_reaction_smiles, count_boundary_atoms_products_and_calculate_changes, calculate_chemical_properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.analysis_process import AnalysisProcess\n",
    "list_data = ['golden_dataset', 'Jaworski', 'USPTO_random_class', 'USPTO_diff', 'USPTO_unbalance_class']\n",
    "pipeline_path = '../../../Pipeline'\n",
    "data_path = '../../../Data'\n",
    "process = AnalysisProcess(list_data, pipeline_path, data_path)\n",
    "\n",
    "data = process.process_and_combine_datasets(remove_undetected=True)\n",
    "data_all = process.standardize_columns(data)\n",
    "data_all = data_all.drop_duplicates(subset=['reactions'])\n",
    "from SynAnalysis.eda_analysis import EDAVisualizer\n",
    "\n",
    "columns = ['carbon_difference', 'fragment_count', 'Bond Changes', 'bond_change_merge', 'ring_change_merge', 'num_boundary']\n",
    "titles = ['Carbon imbalance', 'Total number of substances', 'Number of bond changes', 'Post-MCS bond bariation count', \n",
    "          'Post-MCS ring variation count', 'Number of boundaries']\n",
    "visualizer = EDAVisualizer(data_all, columns, titles)\n",
    "visualizer.visualize_accuracy(chart_type='bar', show_values=False, error_bar=True, save_path='./EDA.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model performance compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# Dataset                     Reactions      Rule Suc.       MCS\n",
    "# Suc.       MCS Acc.\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Jaworski                   637 ( 335)    181  89.60%    127 82.47%   \n",
    "# 121  95.28%\n",
    "# golden_dataset            1851 (1642)    754  93.55%    721 81.19%   \n",
    "# 588  81.55%\n",
    "# USPTO_unbalance_class      540 ( 540)    240  97.96%    298 99.33%   \n",
    "# 289  96.98%\n",
    "# USPTO_random_class         803 ( 803)    324  99.69%    479 100.00%   \n",
    "# 476  99.37%\n",
    "# USPTO_diff                1589 (1589)   1134  96.10%    451 99.12%   \n",
    "# 437  96.90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = {\n",
    "    \"Dataset\": [\"Jaworski\", \"Golden dataset\", \"$\\mathcal{U}_{\\mathrm{Unbalance}}$\", \"$\\mathcal{U}_{\\mathrm{Random}}$\", \"$\\mathcal{U}_{\\mathrm{Diff}}$\"],\n",
    "\n",
    "    \"Reactions\": [637, 1851, 540, 803, 1589],\n",
    "    \"Unbalance\": [335, 1642, 540, 803, 1589],\n",
    "    \"Rule_Suc\": [181, 754, 240, 324, 1134],\n",
    "    \"Rule_Suc_Percentage\": [89.60, 93.55, 97.96, 99.69, 96.10],\n",
    "    \"Rule_Acc\": [181, 754, 240, 324, 1134],\n",
    "    \"Rule_Acc_Percentage\": [89.60, 93.55, 97.96, 99.69, 96.10],\n",
    "    \"MCS_Suc\": [127, 721, 298, 479, 451],\n",
    "    \"MCS_Suc_Percentage\": [82.47, 81.19, 99.33, 100.00, 99.12],\n",
    "    \"MCS_Acc\": [121, 588, 289, 476, 437],\n",
    "    \"MCS_Acc_Percentage\": [95.28, 81.55, 96.98, 99.37, 96.90]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to pandas DataFrame for further analysis\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Rule_Suc_Percentage'] = df['Rule_Suc_Percentage'] /100\n",
    "df['Rule_Acc_Percentage'] = np.array([98.90, 99.73, 99.58, 99.38, 99.91])/100\n",
    "df['Rule_Acc'] = np.array([179, 752, 239, 322, 1133])\n",
    "\n",
    "\n",
    "df['MCS_Suc_Percentage'] = df['MCS_Suc_Percentage'] /100\n",
    "df['MCS_Acc_Percentage'] = df['MCS_Acc_Percentage'] /100\n",
    "\n",
    "df['All_Suc'] = df['Rule_Suc'] + df['MCS_Suc']\n",
    "df['All_Suc_Percentage'] = df['All_Suc'] / df['Unbalance']\n",
    "df['All_Acc'] = df['Rule_Acc'] + df['MCS_Acc']\n",
    "df['All_Acc_Percentage'] = df['All_Acc'] / df['All_Suc'] \n",
    "\n",
    "\n",
    "df['Sample_rule'] = np.round(df['Rule_Suc'] / df['Rule_Suc_Percentage'],0)\n",
    "df['Sample_mcs'] = np.round(df['MCS_Suc'] / df['MCS_Suc_Percentage'],0)\n",
    "df['Sample_mcs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset                     Reactions      Rule Suc.       MCS\n",
    "# Suc.       MCS Acc.    MCS Suc. >50%    MCS Acc. >50%\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Jaworski                   637 ( 335)    181  89.60%    127 82.47%   \n",
    "# 121  95.28%       22  66.67%       22 100.00%\n",
    "# golden_dataset            1851 (1642)    754  93.55%    721 81.19%   \n",
    "# 588  81.55%      115  65.34%      104  90.43%\n",
    "# USPTO_unbalance_class      540 ( 540)    240  97.96%    298 99.33%   \n",
    "# 289  96.98%       59  96.72%       58  98.31%\n",
    "# USPTO_random_class         803 ( 803)    324  99.69%    479 100.00%   \n",
    "# 476  99.37%       94  98.95%       94 100.00%\n",
    "# USPTO_diff                1589 (1589)   1134  96.10%    451 99.12%   \n",
    "# 437  96.90%       86  95.56%       86 100.00%\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_updated = {\n",
    "    \"Dataset\": [\"Jaworski\", \"golden_dataset\", \"USPTO_unbalance_class\", \"USPTO_random_class\", \"USPTO_diff\"],\n",
    "    \"Reactions\": [637, 1851, 540, 803, 1589],\n",
    "    \"Reactions_number\": [335, 1642, 540, 803, 1589],\n",
    "    \"MCS_Success\": [22, 115, 59, 94, 86],\n",
    "    \"MCS_Success_Percentage\": [66.67, 65.34, 96.72, 98.95, 95.56],\n",
    "    \"MCS_Accuracy\": [22, 104, 58, 94, 86],\n",
    "    \"MCS_Accuracy_Percentage\": [100.00, 90.43, 98.31, 100.00, 100.00],\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame with updated data\n",
    "new_data = pd.DataFrame(data_updated)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['Constraint_Suc_Percentage'] = new_data['MCS_Success_Percentage']/100\n",
    "df['Constraint_Acc_Percentage'] = new_data['MCS_Accuracy_Percentage']/100\n",
    "df['Sample_constraint']= np.round(new_data['MCS_Success'] / (new_data['MCS_Success_Percentage']/100),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.visualizer import barplot_accuracy_comparison_2x2\n",
    "rule_based = df[['Dataset', 'Rule_Suc_Percentage', 'Rule_Acc_Percentage', 'Sample_rule']].rename(columns={'Rule_Suc_Percentage': 'Success Rate', 'Rule_Acc_Percentage': 'Accuracy', 'Sample_rule': 'Unbalance'})\n",
    "mcs_based = df[['Dataset', 'MCS_Suc_Percentage', 'MCS_Acc_Percentage', 'Sample_mcs']].rename(columns={'MCS_Suc_Percentage': 'Success Rate', 'MCS_Acc_Percentage': 'Accuracy', 'Sample_mcs': 'Unbalance'})\n",
    "all_data = df[['Dataset', 'All_Suc_Percentage', 'All_Acc_Percentage', 'Unbalance']].rename(columns={'All_Suc_Percentage': 'Success Rate', 'All_Acc_Percentage': 'Accuracy'})\n",
    "constraint = df[['Dataset', 'Constraint_Suc_Percentage', 'Constraint_Acc_Percentage', 'Sample_constraint']].rename(columns={'Constraint_Suc_Percentage': 'Success Rate', 'Constraint_Acc_Percentage': 'Accuracy', 'Sample_constraint': 'Unbalance'})\n",
    "barplot_accuracy_comparison_2x2([rule_based, all_data, mcs_based, constraint], show_values=True, \n",
    "                                save_path='accuracy_bar_plot.pdf',\n",
    "                                title_names = ['Rule-based', 'Overall', 'MCS-based', 'Confidence-constraint'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MCS Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynRBL.SynMCSImputer.SubStructure.mcs_process import ensemble_mcs\n",
    "from SynRBL.SynUtils.data_utils import load_database, save_database\n",
    "from SynRBL.SynMCSImputer.SubStructure.extract_common_mcs import ExtractMCS\n",
    "from SynRBL.SynMCSImputer.MissingGraph.find_graph_dict import find_graph_dict\n",
    "from SynRBL.SynMCSImputer.MissingGraph.refinement_uncertainty import RefinementUncertainty\n",
    "\n",
    "mcs1 = load_database('../../../Data/Validation_set/golden_dataset/MCS/Condition_1.json.gz')\n",
    "mcs2 = load_database('../../../Data/Validation_set/golden_dataset/MCS/Condition_2.json.gz')\n",
    "mcs3 = load_database('../../../Data/Validation_set/golden_dataset/MCS/Condition_3.json.gz')\n",
    "mcs4 = load_database('../../../Data/Validation_set/golden_dataset/MCS/Condition_4.json.gz')\n",
    "mcs5 = load_database('../../../Data/Validation_set/golden_dataset/MCS/Condition_5.json.gz')\n",
    "datasets = [\"Configuration 1\", \"Configuration 2\", \"Configuration 3\", \"Configuration 4\", \"Configuration 5\", 'Ensemble']\n",
    "golden = [256, 279, 211, 269, 398, 195]\n",
    "jaworski = [26, 30, 22, 33, 46, 21]\n",
    "random = [66, 80, 60, 75, 203, 60]\n",
    "diff = [44, 50, 42, 45, 128, 40]\n",
    "unbalance = [37, 37, 33, 35, 130, 33]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    \"Configuration\": [\"Configuration 1\", \"Configuration 2\", \"Configuration 3\", \"Configuration 4\", \"Configuration 5\", 'Ensemble'] * 5,\n",
    "    \"Dataset\": [\"Golden dataset\"] * 6 + [\"Jaworski\"] * 6 + [\"$\\mathcal{U}_{\\mathrm{Random}}$\"] * 6 + [\"$\\mathcal{U}_{\\mathrm{Diff}}$\"] * 6 + [\"$\\mathcal{U}_{\\mathrm{Unbalance}}$\"] * 6,\n",
    "    \"Value\": [256, 279, 211, 269, 398, 195, 26, 30, 22, 33, 46, 21, 66, 80, 60, 75, 203, 60, 44, 50, 42, 45, 128, 40, 37, 37, 33, 35, 130, 33]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.visualizer import mcs_comparsion\n",
    "mcs_comparsion([mcs1, mcs1, mcs2, mcs3, mcs4, mcs5], df, 67, save_path='./comparison_mcs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.visualizer import mcs_comparsion\n",
    "mcs_comparsion([mcs1, mcs1, mcs2, mcs3, mcs4, mcs5], df, 67, save_path='./comparison_mcs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.analysis_process import AnalysisProcess\n",
    "list_data = ['golden_dataset', 'Jaworski', 'USPTO_random_class', 'USPTO_diff', 'USPTO_unbalance_class']\n",
    "pipeline_path = '../../../Pipeline'\n",
    "data_path = '../../../Data'\n",
    "process = AnalysisProcess(list_data, pipeline_path, data_path)\n",
    "\n",
    "data_check = process.process_and_combine_datasets(remove_undetected=False).drop(['R-id', 'reactions','Index', 'mcs_carbon_balanced'], axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "import seaborn as sns\n",
    "\n",
    "class FeatureAnalysis:\n",
    "    def __init__(self, data: pd.DataFrame, target_col: str, cols_for_contour: List[List[str]], figsize: tuple = (16, 12)):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureAnalysis class.\n",
    "\n",
    "        Parameters:\n",
    "        - data (pd.DataFrame): The DataFrame containing the data.\n",
    "        - target_col (str): The name of the target column.\n",
    "        - cols_for_contour (List[List[str]]): List of lists containing feature pairs for contour plots.\n",
    "        - figsize (tuple): Figure size for the visualization. Default is (16, 12).\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.cols_for_contour = cols_for_contour\n",
    "        self.figsize = figsize\n",
    "\n",
    "    def feature_importance(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"\n",
    "        Create a feature importance plot using XGBoost.\n",
    "\n",
    "        Parameters:\n",
    "        - ax (plt.Axes): The axes on which to plot the feature importance.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        X = self.data.drop(self.target_col, axis=1)\n",
    "        y = self.data[self.target_col]\n",
    "\n",
    "        clf = xgb.XGBClassifier(random_state=42)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        feature_importances = clf.feature_importances_\n",
    "        importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        cmap = plt.get_cmap('flare')\n",
    "        colors = [cmap(i / len(importance_df)) for i in range(len(importance_df))]\n",
    "        bars = ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "\n",
    "        for bar, val in zip(bars, importance_df['Importance']):\n",
    "            ax.text(val + 0.002, bar.get_y() + bar.get_height() / 2, f'{val:.3f}', va='center', fontsize=15, color='black')\n",
    "\n",
    "        ax.set_xlabel('Importance', fontsize=20)\n",
    "        ax.set_yticks(range(len(X.columns)))\n",
    "        ax.set_yticklabels(importance_df['Feature'], fontsize=20)\n",
    "        #ax.set_ylabel('Importance', fontsize=20)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "\n",
    "    def contour_plot(self, features: List[str], ax: plt.Axes) -> None:\n",
    "        \"\"\"\n",
    "        Create a contour plot for a pair of features.\n",
    "\n",
    "        Parameters:\n",
    "        - features (List[str]): List of two feature names.\n",
    "        - ax (plt.Axes): The axes on which to plot the contour plot.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        df = deepcopy(self.data)\n",
    "        le = LabelEncoder()\n",
    "        df['Outcome'] = le.fit_transform(df[self.target_col])\n",
    "\n",
    "        X = df[list(features)]\n",
    "        y = df['Outcome']\n",
    "\n",
    "        model = XGBClassifier(random_state=42)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        x_min, x_max = X[features[0]].min() - 1, X[features[0]].max() + 1\n",
    "        y_min, y_max = X[features[1]].min() - 1, X[features[1]].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))  # Increase the number of points\n",
    "\n",
    "        Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Create filled contour plot with increased size\n",
    "        contour = ax.contourf(xx, yy, Z, alpha=0.8, levels=np.linspace(0, 1, 11), cmap=plt.cm.coolwarm)\n",
    "\n",
    "        ax.set_xlabel(features[0], fontsize=20)\n",
    "        ax.set_ylabel(features[1], fontsize=20)\n",
    "\n",
    "        # Add colorbar for the probability values\n",
    "        cbar = plt.colorbar(contour, ax=ax, orientation='vertical', label='Accuracy')\n",
    "\n",
    "    def visualize(self, save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Visualize feature importance and contour plots.\n",
    "\n",
    "        Parameters:\n",
    "        - save_path (Optional[str]): Path to save the figure. If None, the figure is not saved. Default is None.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=self.figsize)\n",
    "        gs = gridspec.GridSpec(3, 3, figure=fig)\n",
    "        \n",
    "        # Feature importance plot (A)\n",
    "        ax_feature_importance = fig.add_subplot(gs[:, :2])\n",
    "        self.feature_importance(ax_feature_importance)\n",
    "\n",
    "        # Label for feature importance plot (A)\n",
    "        labels_A = ['A'] \n",
    "        for ax, label in zip([ax_feature_importance], labels_A):\n",
    "            ax.text(-0.1, 1.01, label, transform=ax.transAxes, size=20, weight='bold', va='top', ha='right')\n",
    "\n",
    "        labels_BCD = ['B', 'C', 'D']  # Labels for contour plots (B, C, D)\n",
    "\n",
    "        # Assign labels to contour plots (B, C, D)\n",
    "        for k, cols in enumerate(self.cols_for_contour):\n",
    "            ax = fig.add_subplot(gs[k, 2])\n",
    "            self.contour_plot(cols, ax)\n",
    "            ax.text(-0.1, 1.04, labels_BCD[k], transform=ax.transAxes, size=24, weight='bold', va='top', ha='right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=600, transparent=True, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from SynAnalysis.feature_analysis import FeatureAnalysis\n",
    "vis = FeatureAnalysis(data_check, 'Result', [('ring_change_merge','num_boundary'),\n",
    "                                           ('fragment_count','num_boundary'), \n",
    "                                           ('ring_change_merge','fragment_count')])\n",
    "vis.visualize(save_path='./fearure_importance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_database('../../../Data/Validation_set/mcs_based_reactions_train.json.gz')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "# from imblearn.pipeline import Pipeline as Pipeline\n",
    "# from SynAnalysis.analysis_process import AnalysisProcess\n",
    "# import pickle\n",
    "# list_data = ['golden_dataset', 'Jaworski', 'USPTO_random_class', 'USPTO_diff', 'USPTO_unbalance_class']\n",
    "# pipeline_path = '../../../Pipeline'\n",
    "# data_path = '../../../Data'\n",
    "# process = AnalysisProcess(list_data, pipeline_path, data_path)\n",
    "# data_raw = process.process_and_combine_datasets(remove_undetected=False).drop(['R-id', 'reactions', 'Bond Changes',\n",
    "#                                                                                 'Index', 'mcs_carbon_balanced'], axis =1)\n",
    "\n",
    "# data_raw = data_raw.drop_duplicates()\n",
    "# X, y = data_raw.drop('Result', axis=1), data_raw['Result']\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# steps = [('scaler', MinMaxScaler()), ('over', SMOTETomek(sampling_strategy='minority', random_state=42)), ('model', XGBClassifier(random_state=42,))]\n",
    "# pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# with open('scoring_function.pkl', 'wb') as file:\n",
    "#     pickle.dump(pipeline, file)\n",
    "\n",
    "# # Print classification report\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw = process.process_and_combine_datasets(remove_undetected=False).drop(['reactions', 'Bond Changes',\n",
    "#                                                                                 'Index', 'mcs_carbon_balanced'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynRBL.rsmi_utils import load_database, save_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mcs = load_database('../../../Data/Validation_set/mcs_based_reactions_train.json.gz')\n",
    "train_merge = load_database('../../../Data/Validation_set/MCS_Impute_train.json.gz')\n",
    "data_all = pd.read_csv('../../../Pipeline/Validation/Analysis/final_validation.csv')\n",
    "data_all['Result']=data_all['correct_reaction'].notnull()\n",
    "data_all = data_all[['R-id', 'Result']]\n",
    "data_all = data_all.to_dict('records')\n",
    "r_id = [train_mcs[key]['R-id'] for key, value in enumerate(train_mcs)]\n",
    "result = [value for key, value in enumerate(data_all) if value['R-id'] in r_id]\n",
    "train_mcs_df = pd.DataFrame(train_mcs)\n",
    "train_mcs_df.index = train_mcs_df ['R-id']\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.index = result_df['R-id']\n",
    "train_mcs_df = pd.concat([train_mcs_df, result_df], axis=1)\n",
    "train_mcs_df = train_mcs_df.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mcs = load_database('../../../Data/Validation_set/mcs_based_reactions_test.json.gz')\n",
    "test_merge = load_database('../../../Data/Validation_set/MCS_Impute_test.json.gz')\n",
    "data_all = pd.read_csv('../../../Pipeline/Validation/Analysis/final_validation.csv')\n",
    "data_all['Result']=data_all['correct_reaction'].notnull()\n",
    "data_all = data_all[['R-id', 'Result']]\n",
    "data_all = data_all.to_dict('records')\n",
    "r_id = [test_mcs[key]['R-id'] for key, value in enumerate(test_mcs)]\n",
    "result = [value for key, value in enumerate(data_all) if value['R-id'] in r_id]\n",
    "test_mcs_df = pd.DataFrame(test_mcs)\n",
    "test_mcs_df.index = test_mcs_df ['R-id']\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.index = result_df['R-id']\n",
    "test_mcs_df = pd.concat([test_mcs_df, result_df], axis=1)\n",
    "test_mcs_df = test_mcs_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.analysis_utils import remove_atom_mapping_from_reaction_smiles,calculate_chemical_properties, count_boundary_atoms_products_and_calculate_changes\n",
    "from IPython.display import clear_output\n",
    "def process_data(merge_data, mcs_data, remove_undetected=False):\n",
    "    merge_data = count_boundary_atoms_products_and_calculate_changes(merge_data)\n",
    "    mcs_data = calculate_chemical_properties(mcs_data)\n",
    "    combined_data = pd.concat([\n",
    "                pd.DataFrame(mcs_data)[['R-id', 'carbon_difference', 'fragment_count', 'total_carbons', 'total_bonds', 'total_rings', 'Result']],\n",
    "                pd.DataFrame(merge_data)[['mcs_carbon_balanced', 'num_boundary', 'ring_change_merge', 'bond_change_merge']],\n",
    "            ], axis=1)\n",
    "    if remove_undetected:\n",
    "        combined_data = combined_data[combined_data['mcs_carbon_balanced'] == True]\n",
    "\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "    unnamed_columns = [col for col in combined_data.columns if 'Unnamed' in col]\n",
    "    combined_data = combined_data.drop(unnamed_columns, axis=1)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = process_data(merge_data=train_merge, mcs_data=train_mcs_df, remove_undetected=False).drop(['R-id', 'mcs_carbon_balanced'], axis =1)  \n",
    "data_test =process_data(merge_data=test_merge, mcs_data=test_mcs_df, remove_undetected=False).drop(['R-id', 'mcs_carbon_balanced'], axis =1)\n",
    "\n",
    "X_train, y_train = data_train.drop('Result', axis=1), data_train['Result']\n",
    "X_test, y_test = data_test.drop('Result', axis=1), data_test['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as Pipeline\n",
    "from SynAnalysis.analysis_process import AnalysisProcess\n",
    "import pickle\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "steps = [('scaler', MinMaxScaler()), ('over', SMOTETomek(sampling_strategy='minority', random_state=42)), ('model', XGBClassifier(random_state=42,))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "with open('scoring_function.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from typing import List, Optional\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "def classification_visualization(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray, save_path: str = None, figsize: tuple = (14, 14)):\n",
    "    \"\"\"\n",
    "    Visualize classification metrics including Confusion Matrix, Classification Report, ROC Curve, and Precision-Recall Curve.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (np.ndarray): True labels.\n",
    "    y_pred (np.ndarray): Predicted labels.\n",
    "    y_proba (np.ndarray): Predicted probabilities.\n",
    "    save_path (str, optional): Path to save the figure. If None, the figure is not saved. Default is None.\n",
    "    figsize (tuple, optional): Figure size (width, height). Default is (14, 14).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Setup the matplotlib figure and axes, 2x2 layout\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    #fig.suptitle('Advanced Classification Metrics Visualization', fontsize=16)\n",
    "\n",
    "    labels = ['A', 'B', 'C', 'D']  # Labels for each subplot\n",
    "    for ax, label in zip(axes.flat, labels):\n",
    "        ax.text(-0.1, 1.1, label, transform=ax.transAxes, size=20, weight='bold', va='top', ha='right')\n",
    "\n",
    "    # Subfig 1: Confusion Matrix\n",
    "    ax = axes[0, 0]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, annot_kws={\"size\": 14})\n",
    "    ax.set(xlabel='Predicted labels', ylabel='True labels')\n",
    "    ax.set_title('Confusion Matrix', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "    # Subfig 2: Classification Report\n",
    "    ax = axes[0, 1]\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    sns.heatmap(report_df.iloc[:-1, :].astype(float), annot=True, cmap='Spectral', cbar=True, fmt=\".2f\", ax=ax, annot_kws={\"size\": 14})\n",
    "    ax.set_title('Classification Report', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "    # Enhance ROC Curve visual\n",
    "    ax = axes[1, 0]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba[:, 0], pos_label=0)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % roc_auc, color='darkorange', lw=2)\n",
    "    ax.fill_between(fpr, tpr, color='darkorange', alpha=0.3)\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='navy')\n",
    "    ax.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05], xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax.set_title('ROC Curve', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Enhance Precision-Recall Curve visual\n",
    "    ax = axes[1, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba[:,0], pos_label=0)\n",
    "    average_precision = average_precision_score(y_true, y_proba[:,0], pos_label=0)\n",
    "    ax.plot(recall, precision, label='Precision-Recall curve (AP = %0.2f)' % average_precision, color='blue', lw=2)\n",
    "    ax.fill_between(recall, precision, color='blue', alpha=0.3)\n",
    "    ax.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05], xlabel='Recall', ylabel='Precision')\n",
    "    ax.set_title('Precision-Recall Curve', fontsize=18, weight='bold', pad=20)\n",
    "\n",
    "    ax.legend(loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, transparent=True, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from SynAnalysis.visualizer import classification_visualization\n",
    "\n",
    "classification_visualization(y_test, y_pred, pipeline.predict_proba(X_test),\n",
    "                             save_path = 'model_performance.png', figsize = (14, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from pandas import DataFrame\n",
    "from SynRBL.rsmi_utils import load_database\n",
    "import pickle\n",
    "from SynAnalysis.analysis_utils import remove_atom_mapping_from_reaction_smiles, calculate_chemical_properties, count_boundary_atoms_products_and_calculate_changes\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def confidence_level(merge_data_path: str, mcs_data_path: str, scoring_function_path: str, remove_undetected: bool = True) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculates the confidence level for chemical reactions based on their properties and a pre-trained model.\n",
    "    \n",
    "    This function loads merged and MCS (Maximum Common Substructure) reaction data, calculates various chemical\n",
    "    properties, and uses a pre-trained model to predict a confidence level for each reaction.\n",
    "    \n",
    "    Parameters:\n",
    "    - merge_data_path (str): Path to the file containing merged reaction data.\n",
    "    - mcs_data_path (str): Path to the file containing MCS reaction data.\n",
    "    - scoring_function_path (str): Path to the pre-trained model file (pickle format).\n",
    "    - remove_undetected (bool, optional): If True, removes reactions where the MCS carbon balance is not detected. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    - List[float]: A list of confidence scores for each reaction, based on the predictions from the pre-trained model.\n",
    "    \n",
    "    Note:\n",
    "    - The function assumes that the reaction data includes specific fields such as 'R-id' for reaction ID and chemical property columns.\n",
    "    - The pre-trained model should be capable of providing probability estimates through a `predict_proba` method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and process merge data\n",
    "    merge_data = load_database(merge_data_path)\n",
    "    merge_data = count_boundary_atoms_products_and_calculate_changes(merge_data)\n",
    "    \n",
    "    # Load and process MCS data\n",
    "    mcs_data = load_database(mcs_data_path)\n",
    "    id = [value['R-id'] for value in merge_data]\n",
    "    mcs_data = [value for value in mcs_data if value['R-id'] in id]\n",
    "    mcs_data = calculate_chemical_properties(mcs_data)\n",
    "    \n",
    "    # Clear output\n",
    "    clear_output(wait=False)\n",
    "    \n",
    "    # Combine data and filter if necessary\n",
    "    combined_data = pd.concat([\n",
    "        pd.DataFrame(mcs_data)[['R-id', 'reactions', 'carbon_difference', 'fragment_count', 'total_carbons', 'total_bonds', 'total_rings']],\n",
    "        pd.DataFrame(merge_data)[['mcs_carbon_balanced', 'num_boundary', 'ring_change_merge', 'bond_change_merge', 'new_reaction']],\n",
    "    ], axis=1)\n",
    "    \n",
    "    # if remove_undetected:\n",
    "    #     combined_data = combined_data[combined_data['mcs_carbon_balanced'] == True]\n",
    "    \n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "    unnamed_columns = [col for col in combined_data.columns if 'Unnamed' in col]\n",
    "    combined_data = combined_data.drop(unnamed_columns, axis=1)\n",
    "    \n",
    "    # Prepare data for prediction\n",
    "    X_pred = combined_data[['carbon_difference', 'fragment_count', 'total_carbons', 'total_bonds', 'total_rings', 'num_boundary', 'ring_change_merge', 'bond_change_merge']]\n",
    "    \n",
    "    # Load model and predict confidence\n",
    "    with open(scoring_function_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    confidence = np.round(loaded_model.predict_proba(X_pred)[:, 1],3)\n",
    "    combined_data['confidence'] = confidence\n",
    "    \n",
    "    return combined_data[['R-id', 'reactions', 'new_reaction', 'confidence', 'mcs_carbon_balanced']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_all = ['golden_dataset', 'Jaworski', 'USPTO_random_class', 'USPTO_diff', 'USPTO_unbalance_class']\n",
    "data_all = pd.DataFrame()\n",
    "for data_name in list_data_all:\n",
    "    result_df = pd.read_csv(f'./SynRBL - {data_name}.csv')[['Result']]\n",
    "    result_df.loc[result_df['Result'] == 'CONSIDER', 'Result'] = False\n",
    "    result_df.loc[result_df['Result'] == 'FALSE', 'Result'] = False\n",
    "    result_df.loc[result_df['Result'] == 'TRUE', 'Result'] = True\n",
    "    data_pred = confidence_level(merge_data_path= f'../../../Data/Validation_set/{data_name}/MCS/MCS_Impute.json.gz', \n",
    "                              mcs_data_path = f'../../../Data/Validation_set/{data_name}/mcs_based_reactions.json.gz', \n",
    "                              scoring_function_path=f'./scoring_function.pkl', remove_undetected=True)\n",
    "    data_pred=data_pred.rename(columns={'mcs_carbon_balanced': 'Success'})\n",
    "    data_pred = pd.concat([data_pred, result_df], axis=1)\n",
    "    data_all = pd.concat([data_all, data_pred], axis=0)\n",
    "    data_all = data_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['reactions'] = data_all['reactions'].apply(lambda x: remove_atom_mapping_from_reaction_smiles(x))\n",
    "data_all = data_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_all[['Success','Result','confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sucess_rate(data):\n",
    "    rate = []\n",
    "    for key, value in enumerate(data['confidence']):\n",
    "        data_temp = deepcopy(data)\n",
    "        #data_test = data.loc[data['confidence'] >= value, :]\n",
    "        data_temp.loc[data_temp['confidence'] < value, 'Success'] = False\n",
    "        sucess_rate = len(data_temp.loc[data_temp['Success'] == True, :]) / len(data_temp)\n",
    "        rate.append(sucess_rate)\n",
    "    return rate\n",
    "\n",
    "def accuracy_rate(data):\n",
    "    rate = []\n",
    "    for key, value in enumerate(data['confidence']):\n",
    "        data_temp = deepcopy(data)\n",
    "        #data_test = data.loc[data['confidence'] >= value, :]\n",
    "        data_temp = data_temp.loc[data_temp['confidence'] >= value, :]\n",
    "        accuracy_rate = len(data_temp.loc[data_temp['Result'] == True, :]) / len(data_temp)\n",
    "        rate.append(sucess_rate)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_sorted = df.sort_values(by='confidence')\n",
    "# Assuming 'data' is the DataFrame 'df'\n",
    "data = deepcopy(df_sorted)  # Using df_sorted as the base data for this example\n",
    "\n",
    "def calculate_rates(data):\n",
    "    success_rates = []\n",
    "    accuracy_rates = []\n",
    "    unique_confidences = np.sort(data['confidence'].unique())\n",
    "    \n",
    "    for value in unique_confidences:\n",
    "        # Mark predictions as unsuccessful if below the current threshold\n",
    "        deemed_successful = data['confidence'] >= value\n",
    "        success_rate = deemed_successful.mean()\n",
    "        \n",
    "        # For accuracy, consider only those deemed successful\n",
    "        if deemed_successful.any():  # Check if there's at least one deemed successful\n",
    "            correct_predictions = data.loc[deemed_successful, 'Result'] == True\n",
    "            accuracy_rate = correct_predictions.mean()\n",
    "        else:\n",
    "            accuracy_rate = 0  # No predictions are deemed successful\n",
    "        \n",
    "        success_rates.append(success_rate)\n",
    "        accuracy_rates.append(accuracy_rate)\n",
    "    \n",
    "    return success_rates, accuracy_rates, unique_confidences\n",
    "\n",
    "# Calculate the rates\n",
    "success_rates, accuracy_rates, unique_confidences = calculate_rates(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_mess(success_rates, accuracy_rate):\n",
    "    success_rates = np.array(success_rates)\n",
    "    accuracy_rate = np.array(accuracy_rate)\n",
    "    return (2 * success_rates * accuracy_rate) / (success_rates + accuracy_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "def plot_success_vs_accuracy(success_rates, accuracy_rates, thresholds):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(success_rates, accuracy_rates, linestyle='-')\n",
    "    plt.xlabel('Success Rate')\n",
    "    plt.ylabel('Accuracy Rate')\n",
    "    plt.title('Accuracy Rate vs. Success Rate with Confidence Thresholds')\n",
    "    plt.grid(True)\n",
    "    f_messure = f_mess(success_rates, accuracy_rates)\n",
    "\n",
    "    # Annotate some key points with their corresponding confidence threshold\n",
    "    index = np.argmax(f_messure)\n",
    "    # #annotation_indices = np.linspace(0, len(thresholds) - 1, min(len(thresholds), 10), dtype=int)  # Up to 10 evenly spaced points\n",
    "    # #for i in annotation_indices:\n",
    "    print(thresholds[index])\n",
    "    print(success_rates[index])\n",
    "    print(accuracy_rates[index])\n",
    "    plt.annotate(f'{thresholds[index]:.2f}', (success_rates[index], accuracy_rates[index]),\n",
    "                    textcoords=\"offset points\", xytext=(-10,-10),\n",
    "                    ha='center', color='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Now, we'll use this function to plot the data and try to identify the optimal threshold\n",
    "plot_success_vs_accuracy(success_rates, accuracy_rates, unique_confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(unique_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_rate(data):\n",
    "    rate = []\n",
    "    for key, value in enumerate(data['confidence']):\n",
    "        data_temp = deepcopy(data)\n",
    "        #data_test = data.loc[data['confidence'] >= value, :]\n",
    "        data_temp = data_temp.loc[data_temp['confidence'] >= value, :]\n",
    "        accuracy_rate = len(data_temp.loc[data_temp['Result'] == True, :]) / len(data_temp)\n",
    "        rate.append(sucess_rate)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = sucess_rate(df)\n",
    "accuracy_rate = accuracy_rate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store success rate and accuracy rate\n",
    "success_rate = []\n",
    "accuracy_rate = []\n",
    "\n",
    "# Unique sorted list of confidence thresholds\n",
    "unique_thresholds = np.unique(df_sorted['confidence'])\n",
    "\n",
    "for threshold in unique_thresholds:\n",
    "    # Consider a prediction successful if its confidence is >= threshold\n",
    "    deemed_successful = df_sorted[df_sorted['confidence'] >= threshold]\n",
    "    \n",
    "    # Success rate: Proportion of predictions deemed successful\n",
    "    success_rate.append(len(deemed_successful) / len(df_sorted))\n",
    "    \n",
    "    # Accuracy rate: Proportion of correct predictions among the deemed successful\n",
    "    if len(deemed_successful) > 0:\n",
    "        accuracy = np.mean(deemed_successful['Result'] == deemed_successful['Success'])\n",
    "    else:\n",
    "        accuracy = 0  # No predictions are deemed successful at this threshold\n",
    "    \n",
    "    accuracy_rate.append(accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(unique_thresholds, success_rate, label='Success Rate', color='green')\n",
    "plt.plot(unique_thresholds, accuracy_rate, label='Accuracy Rate', color='orange')\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Success Rate and Accuracy Rate vs. Confidence Threshold')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_all['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data, replace this with the user's actual DataFrame\n",
    "data = {\n",
    "    'Success': [True, True, True, True, True, True, True, True, True, True, True],\n",
    "    'Result': [False, True, True, True, False, True, True, True, True, True, True],\n",
    "    'confidence': [0.127, 0.996, 0.999, 0.993, 0.151, 0.990, 0.726, 0.990, 0.912, 0.996, 0.996]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by confidence\n",
    "df_sorted = df.sort_values(by='confidence')\n",
    "\n",
    "# Calculate TPR and FPR at each threshold\n",
    "thresholds = df_sorted['confidence']\n",
    "TPR = []  # True Positive Rate\n",
    "Precision = []  # Precision\n",
    "\n",
    "for threshold in thresholds:\n",
    "    TP = ((df_sorted['Result'] == True) & (df_sorted['confidence'] >= threshold)).sum()\n",
    "    FP = ((df_sorted['Result'] == False) & (df_sorted['confidence'] >= threshold)).sum()\n",
    "    FN = ((df_sorted['Result'] == True) & (df_sorted['confidence'] < threshold)).sum()\n",
    "    \n",
    "    tpr = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "    \n",
    "    TPR.append(tpr)\n",
    "    Precision.append(precision)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# For a more ROC-like curve, we might plot TPR vs. FPR. Here, we show TPR vs. Precision for illustration.\n",
    "plt.plot(thresholds, TPR, label='TPR', color='blue')\n",
    "plt.plot(thresholds, Precision, label='Precision', color='red')\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('TPR and Precision vs. Confidence Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = confidence_level(merge_data_path= '../../../Data/Validation_set/USPTO_diff/MCS/MCS_Impute.json.gz', \n",
    "                              mcs_data_path = '../../../Data/Validation_set/USPTO_diff/mcs_based_reactions.json.gz', \n",
    "                              scoring_function_path='./scoring_function.pkl', remove_undetected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynRBL\n",
    "import pickle\n",
    "data_predict_name = 'USPTO_unbalance_class'\n",
    "input_path = '../../../Pipeline'\n",
    "source_path = '../../../Data'\n",
    "process = AnalysisProcess(pipeline_path=input_path, data_path=source_path, data_predict_name=data_predict_name)\n",
    "combined_data=process.process_predict_datasets()\n",
    "\n",
    "X_pred = combined_data[['carbon_difference', 'fragment_count', 'total_carbons', 'total_bonds',\n",
    "       'total_rings', 'num_boundary', 'ring_change_merge',\n",
    "       'bond_change_merge']]\n",
    "\n",
    "\n",
    "with open('scoring_function.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "confidence = loaded_model.predict_proba(X_pred)[:,1]\n",
    "confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = process.process_and_combine_datasets(remove_undetected=False).drop(['R-id', 'reactions', 'Bond Changes',\n",
    "                                                                                'Index', 'mcs_carbon_balanced'], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "X, y = data_raw.drop('Result', axis=1), data_raw['Result']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "steps = [('scaler', MinMaxScaler()), ('over', SMOTEENN(sampling_strategy='minority', random_state=42)), ('model', XGBClassifier(random_state=42))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = combined_data[['carbon_difference', 'fragment_count', 'total_carbons', 'total_bonds',\n",
    "       'total_rings', 'num_boundary', 'ring_change_merge',\n",
    "       'bond_change_merge']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.analysis_process import AnalysisProcess\n",
    "list_data = ['golden_dataset', 'Jaworski', 'USPTO_random_class', 'USPTO_diff', 'USPTO_unbalance_class']\n",
    "pipeline_path = '../../../Pipeline'\n",
    "data_path = '../../../Data'\n",
    "process = AnalysisProcess(list_data, pipeline_path, data_path)\n",
    "\n",
    "data_check = process.process_and_combine_datasets(remove_undetected=False).drop(['R-id', 'reactions','Index', 'mcs_carbon_balanced'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check_2 = data_check[['bond_change_merge', 'num_boundary', 'fragment_count', 'ring_change_merge', 'Result']]\n",
    "data_check_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from typing import List, Optional\n",
    "\n",
    "class Contourplot:\n",
    "    def __init__(self, data, target_col, figsize=(18, 10)):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.figsize = figsize\n",
    "        features = [col for col in data.columns if col != target_col]\n",
    "        self.cols_for_contour = [list(pair) for pair in combinations(features, 2)]\n",
    "\n",
    "    def contour_plot(self, features: List[str], ax: plt.Axes) -> None:\n",
    "        df = deepcopy(self.data)\n",
    "        le = LabelEncoder()\n",
    "        df['Outcome'] = le.fit_transform(df[self.target_col])\n",
    "\n",
    "        X = df[features]\n",
    "        y = df['Outcome']\n",
    "\n",
    "        model = XGBClassifier(random_state=42)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        x_min, x_max = X[features[0]].min() - 1, X[features[0]].max() + 1\n",
    "        y_min, y_max = X[features[1]].min() - 1, X[features[1]].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "        Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        contour = ax.contourf(xx, yy, Z, alpha=0.8, levels=np.linspace(0, 1, 11), cmap=plt.cm.coolwarm)\n",
    "        ax.set_xlabel(features[0])\n",
    "        ax.set_ylabel(features[1])\n",
    "        cbar = plt.colorbar(contour, ax=ax, orientation='vertical', label='Accuracy')\n",
    "\n",
    "    def visualize(self, save_path: Optional[str] = None) -> None:\n",
    "        fig = plt.figure(figsize=self.figsize)\n",
    "        gs = gridspec.GridSpec(2, 3, figure=fig)\n",
    "       \n",
    "        for k, cols in enumerate(self.cols_for_contour[:6]):\n",
    "            row_idx = k // 3\n",
    "            col_idx = k % 3\n",
    "            ax = fig.add_subplot(gs[row_idx, col_idx])\n",
    "            self.contour_plot(cols, ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=600, transparent=True, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Contourplot(data_check_2, 'Result')\n",
    "test.visualize(save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SynAnalysis.feature_analysis import FeatureAnalysis\n",
    "vis = FeatureAnalysis(data_check, 'Result', [('ring_change_merge','num_boundary'),\n",
    "                                           ('fragment_count','num_boundary'), \n",
    "                                           ('ring_change_merge','fragment_count')])\n",
    "vis.visualize(save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SynRBL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
